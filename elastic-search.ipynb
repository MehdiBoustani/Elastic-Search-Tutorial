{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Elastic Search\n",
    "\n",
    "**Mehdi Boustani** - S221594  \n",
    "**Nicolas Schneiders** - S203005  \n",
    "**Maxim Piron** - S211493  \n",
    "**Andreas Stistrup** - S212891  \n",
    "\n",
    "*Faculty of Applied Sciences, University of Liège*\n",
    "\n",
    "April 28, 2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation & configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Elasticsearch\n",
    "If you don't have Docker installed yet, you can download and install it from the [official website](https://www.docker.com/). \n",
    "\n",
    "Once Docker is running on your machine, launch Elasticsearch using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -p 127.0.0.1:9200:9200 -d --name elasticsearch \\\n",
    "  -e \"discovery.type=single-node\" \\\n",
    "  -e \"xpack.security.enabled=false\" \\\n",
    "  -e \"xpack.license.self_generated.type=trial\" \\\n",
    "  -v \"elasticsearch-data:/usr/share/elasticsearch/data\" \\\n",
    "  docker.elastic.co/elasticsearch/elasticsearch:8.15.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies  \n",
    "Let's install all the necessary Python packages we'll be using throughout this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests       → to interact with the Elasticsearch REST API\n",
    "# elasticsearch  → official Elasticsearch Python client\n",
    "# pandas         → for handling and analyzing tabular data (e.g., dataset exploration)\n",
    "# matplotlib     → for optional data visualization (e.g., query stats or aggregations)\n",
    "\n",
    "!pip install requests elasticsearch==8.15.0 pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connexion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "es = Elasticsearch('http://localhost:9200')\n",
    "info = es.info()\n",
    "\n",
    "print('Connected to ElasticSearch !')\n",
    "pprint(info.body)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data with the bulk api\n",
    "\n",
    "To efficiently load a large dataset into Elasticsearch, we use the Bulk API. This method allows us to insert multiple documents in a single request, which is much faster and more efficient than indexing documents one by one. In this example, we will import the contents of our `apod.json` file—where each element is a document—into a new index called `apod`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"apod.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Prepare the actions for the bulk\n",
    "actions = [\n",
    "    {\n",
    "        \"_index\": \"apod\",\n",
    "        \"_id\": doc[\"title\"], # We use the title as index since it is a unique field (the unicity is important!)\n",
    "        \"_source\": doc\n",
    "    }\n",
    "    for doc in data\n",
    "]\n",
    "\n",
    "# We import the data in bulk\n",
    "try:\n",
    "    helpers.bulk(es, actions)\n",
    "    print(\"Bulk import terminé !\")\n",
    "except  Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic queries in ElasticSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elasticsearch exposes a RESTful API, which means you interact with it using standard HTTP methods. Here are the most common operations:\n",
    "\n",
    "- **GET**: Read a document or perform a search\n",
    "- **POST**: Add a new document\n",
    "- **PUT**: Create or replace a document or an index\n",
    "- **DELETE**: Remove a document or an index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET method\n",
    "\n",
    "The GET method is used to retrieve data from our json file by providing an id. If the document with the specified id doesn't exist, it throws an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    doc = es.get(index=\"apod\", id=\"A Hazy Harvest Moon\")\n",
    "    pprint(doc['_source'])\n",
    "\n",
    "except:\n",
    "    print(\"A document with this id doesn't exist!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POST method\n",
    "\n",
    "The POST method is used to create a new document. When using the index() method without specifying an id, elasticsearch automatically generates one (not the title as the other documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "new_id = \"A New APOD\"\n",
    "\n",
    "new_doc = {\n",
    "    \"date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "    \"title\": new_id,\n",
    "    \"explanation\": \"This is a new document added via POST.\",\n",
    "    \"image_url\": \"https://apod.nasa.gov/apod/image/2410/new_apod.jpg\",\n",
    "    \"authors\": \"Mehdi Boustani\"\n",
    "}\n",
    "\n",
    "res = es.index(index=\"apod\", document=new_doc)\n",
    "\n",
    "print(\"Document added successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PUT method\n",
    "\n",
    "The PUT method is used to create or replace a document at a specified id. If a document with that id already exists, it will be overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_id = \"Replaced APOD\"\n",
    "\n",
    "doc = {\n",
    "    \"date\": \"2024-10-02\",\n",
    "    \"title\": replaced_id,\n",
    "    \"explanation\": \"This document replaces any previous one with the same ID.\",\n",
    "    \"image_url\": \"https://apod.nasa.gov/apod/image/2410/new_apod.jpg\",\n",
    "    \"authors\": \"Mehdi Boustani\"\n",
    "}\n",
    "\n",
    "# Let's replace our previously created document\n",
    "es.index(index=\"apod\", id=doc[\"title\"], document=doc)\n",
    "\n",
    "print(f\"Document with id '{new_id}' replaced by a new document with id '{replaced_id}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DELETE method\n",
    "\n",
    "This method is used to delete a document by its id. The id must be known and specified in the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    es.delete(index=\"apod\", id=replaced_id)\n",
    "    print(f\"Document with ID {replaced_id} deleted.\")\n",
    "\n",
    "except:\n",
    "    print(\"The specified document to delete doesn't exist\")\n",
    "\n",
    "# Delete the entire index (be careful, this is command is irreversible)\n",
    "# es.indices.delete(index=\"apod\")\n",
    "# print(\"Index 'apod' deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPL vs SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elasticsearch doesn't use traditional SQL language to query data, but rather a **DSL (Domain Specific Language)** based on JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main differencies\n",
    "\n",
    "1. **Query Structure**\n",
    "   - **SQL**: Uses a strict syntax with clauses like `SELECT`, `FROM`, `WHERE`\n",
    "   - **DSL**: Uses a nested JSON format, offering more flexibility in how queries are expressed\n",
    "\n",
    "2. **Search Types**\n",
    "   - **SQL**: Focuses mainly on exact matches\n",
    "   - **DSL**: Supports advanced search techniques like full-text search, fuzzy matching, and range queries\n",
    "\n",
    "Let's explore practical examples comparing SQL concepts with Elasticsearch's DSL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full-text search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"title\": \"moon\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = es.search(index=\"apod\", body=query)\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(hit[\"_source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL equivalent:** `SELECT * FROM apod WHERE title LIKE '%moon%'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact match with term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"query\": {\n",
    "        \"term\": {\n",
    "            \"title.keyword\": {\n",
    "                \"value\": \"A Hazy Harvest Moon\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = es.search(index=\"apod\", body=query)\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(hit[\"_source\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL equivalent:** `SELECT * FROM apod WHERE title = 'A Hazy Harvest Moon'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Range Query (Numeric/date filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We filter documents by a date range\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"range\": {\n",
    "            \"date\": {\n",
    "                \"gte\": \"2020-01-01\",\n",
    "                \"lte\": \"2020-01-15\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = es.search(index=\"apod\", body=query)\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(hit[\"_source\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL equivalent:** `SELECT * FROM apod WHERE date BETWEEN '2020-01-01' AND '2020-12-31'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy Query (Typo-tolerant search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typo-tolerant search with fuzzy\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"fuzzy\": {\n",
    "            \"title\": {\n",
    "                \"value\": \"Galaxi\",\n",
    "                \"fuzziness\": \"AUTO\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = es.search(index=\"apod\", body=query)\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(hit[\"_source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL equivalent:** No direct equivalent, similar to a `LIKE` with typos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic search as a search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of Elasticsearch is to empower client workflows to retrieve data from your database using powerful, flexible queries. To customize search behavior, Elasticsearch offers several fine-tuning parameters. In this section, we’ll explore the filter, must, must_not, and should clauses.\n",
    "\n",
    "A key concept here is document scoring. When you run a query, Elasticsearch calculates a relevance score for each candidate document and orders results accordingly. You then return the top n documents based on that ranking. To further control how scores influence ordering, you can use the boost parameter to adjust relevance and achieve custom ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you apply a filter criterion to your query, you define one or more clauses that documents must satisfy to be included. Filters are score-neutral, they don’t alter a document’s relevance score, they only prune out non-matching hits. Below we’ll explore a selection of the most common filter clauses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "index_name = \"apod\"\n",
    "\n",
    "# This query will filter out documents that do not match the date \"2024-09-27\"\n",
    "term_query = {\n",
    "    \"term\": {\"date\": \"2024-09-27\"}\n",
    "}\n",
    "\n",
    "# This query will filter documents with a date between \"2024-09-09\" and \"2024-09-30\"\n",
    "range_query = {\n",
    "    \"range\": {\n",
    "        \"date\": {\"gte\": \"2024-09-09\", \"lte\": \"2024-09-30\"}\n",
    "    }\n",
    "}\n",
    "\n",
    "# This query will filter documents that have a non-null value for the field \"image_url\"\n",
    "exists_query = {\n",
    "    \"exists\": {\"field\": \"note\"}\n",
    "}\n",
    "\n",
    "# This query will filter documents that have the exact term \"David Martinez Delgado et al.\" in the \"authors\" field\n",
    "term_authors_query = {\n",
    "    \"term\": {\"authors.keyword\": \"David Martinez Delgado et al.\"}\n",
    "}\n",
    "\n",
    "# This query will filter documents that have a title starting with \"Comet\"\n",
    "prefix_query = {\n",
    "    \"prefix\": {\"title.keyword\": \"Comet\"}\n",
    "}\n",
    "\n",
    "\n",
    "print(\"=== Term Query on date ===\")\n",
    "res = es.search(index=index_name, body={\"query\": {\"bool\": {\"filter\": [term_query]}}})\n",
    "for hit in res[\"hits\"][\"hits\"]:\n",
    "    pprint(hit[\"_source\"])\n",
    "\n",
    "print(\"\\n=== Range Query on date ===\")\n",
    "res = es.search(index=index_name, body={\"query\": {\"bool\": {\"filter\": [range_query]}}})\n",
    "for hit in res[\"hits\"][\"hits\"]:\n",
    "    pprint(hit[\"_source\"])\n",
    "\n",
    "print(\"\\n=== Exists Query on note ===\")\n",
    "res = es.search(index=index_name, body={\"query\": {\"bool\": {\"filter\": [exists_query]}}})\n",
    "for hit in res[\"hits\"][\"hits\"]:\n",
    "    pprint(hit[\"_source\"])\n",
    "\n",
    "print(\"\\n=== Exact Term Query on authors ===\")\n",
    "res = es.search(index=index_name, body={\"query\": {\"bool\": {\"filter\": [term_authors_query]}}})\n",
    "for hit in res[\"hits\"][\"hits\"]:\n",
    "    pprint(hit[\"_source\"])\n",
    "\n",
    "print(\"\\n=== Prefix Query on title ===\")\n",
    "res = es.search(index=index_name, body={\"query\": {\"bool\": {\"filter\": [prefix_query]}}})\n",
    "for hit in res[\"hits\"][\"hits\"]:\n",
    "    pprint(hit[\"_source\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Must requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The must criterion works much like filter in that it first determines which records are eligible but with one key difference: when a document matches a must clause, its relevance score is increased. You can include multiple must clauses, and they’re combined with a logical AND (i.e., a document must satisfy all of them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "must_title = {\n",
    "    \"match\": {\"title\": \"Comet\"}\n",
    "}\n",
    "\n",
    "must_explanation = {\n",
    "    \"match\": {\"explanation\": \"nebula\"}\n",
    "}\n",
    "\n",
    "print(\"=== Must: Title Contains 'light' (size=2) ===\")\n",
    "res = es.search(\n",
    "    index=index_name,\n",
    "    # The size parameter limits the number of results returned. Default is 10.\n",
    "    body={\n",
    "        \"size\": 2,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [must_title]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "for hit in res[\"hits\"][\"hits\"]:\n",
    "    print(f\"_score={hit['_score']:.2f}\")\n",
    "    pprint(hit[\"_source\"])\n",
    "\n",
    "\n",
    "print(\"\\n=== Must: Title Contains 'Comet' AND Explanation Contains 'nebula' (size=2) ===\")\n",
    "res = es.search(\n",
    "    index=index_name,\n",
    "    body={\n",
    "        \"size\": 2,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    must_title,\n",
    "                    must_explanation\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "for hit in res[\"hits\"][\"hits\"]:\n",
    "    print(f\"_score={hit['_score']:.2f}\")\n",
    "    pprint(hit[\"_source\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first query, you’ll notice the first document’s _score is higher than the second’s—clearly demonstrating how the must clause impacts relevance scoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Must_not requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although its name might imply the opposite of must, must_not actually behaves like a negated filter. Any document that matches a must_not clause is simply removed from the set of candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "must_not_image = {\n",
    "    \"exists\": {\"field\": \"image_url\"}\n",
    "}\n",
    "\n",
    "print(\"=== Example 1: must_not exists image_url (size=2) ===\")\n",
    "res1 = es.search(\n",
    "    index=index_name,\n",
    "    body={\n",
    "        \"size\": 2,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must_not\": [must_not_image]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "for hit in res1[\"hits\"][\"hits\"]:\n",
    "    pprint(hit[\"_source\"])\n",
    "\n",
    "\n",
    "filter_date = {\n",
    "    \"range\": {\n",
    "        \"date\": {\"gte\": \"2024-09-09\", \"lte\": \"2024-09-30\"}\n",
    "    }\n",
    "}\n",
    "must_not_comet = {\n",
    "    \"prefix\": {\"title.keyword\": \"Comet\"}\n",
    "}\n",
    "\n",
    "print(\"\\n=== Example 2: range on date AND must_not prefix 'Comet' (size=2) ===\")\n",
    "res2 = es.search(\n",
    "    index=index_name,\n",
    "    body={\n",
    "        \"size\": 2,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"filter\":   [filter_date],\n",
    "                \"must_not\": [must_not_comet]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "for hit in res2[\"hits\"][\"hits\"]:\n",
    "    # Only docs from 2024-09-27 whose title does NOT start with \"Comet\"\n",
    "    pprint(hit[\"_source\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Should requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The should clause in a bool query implements a logical OR across its clauses:\n",
    "\n",
    "1. **Standalone should (no must clauses)**  \n",
    "   - A document only needs to match at least one should clause to be included.\n",
    "\n",
    "2. **Combined must + should**  \n",
    "   - All must clauses still act as required filters that documents must satisfy every must.  \n",
    "   - Each should clause that matches simply boosts the document’s relevance score; non-matching should clauses do not exclude the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query will filter documents that match at least one the specified conditions\n",
    "query1 = {\n",
    "    \"size\": 2,\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"should\": [\n",
    "                { \"match\": { \"title\": \"Comet\" } },\n",
    "                { \"match\": { \"explanation\": \"nebula\" } },\n",
    "                { \"match\": { \"authors\": \"David Martinez Delgado et al.\" } }\n",
    "            ],\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=== Query 1: Standalone should (title OR explanation) ===\")\n",
    "res1 = es.search(index=index_name, body=query1)\n",
    "for hit in res1[\"hits\"][\"hits\"]:\n",
    "    print(f\"_score={hit['_score']:.2f}\")\n",
    "    pprint(hit[\"_source\"])\n",
    "\n",
    "\n",
    "# This query will filter documents that match the date \"2024-09-27\" and boost the score if the title or explanation matches\n",
    "query2 = {\n",
    "    \"size\": 2,\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                { \"range\": { \"date\": { \"gte\": \"2024-09-20\", \"lte\": \"2024-09-30\" } } }\n",
    "            ],\n",
    "            \"should\": [\n",
    "                { \"match\": { \"title\": \"Comet\" } },\n",
    "                { \"match\": { \"explanation\": \"comet\" } }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n=== Query 2: must range 2024-09-20 to 2024-09-30 + should (boost if title/explanation) ===\")\n",
    "res2 = es.search(index=index_name, body=query2)\n",
    "for hit in res2[\"hits\"][\"hits\"]:\n",
    "    print(f\"_score={hit['_score']:.2f}\")\n",
    "    pprint(hit[\"_source\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want to increase the relevance of certain documents, you attach a positive boost to your query clauses (for example, a match or term query) by adding a boost parameter—this simply multiplies that clause’s score contribution in the final _score. To softly penalize documents without filtering them out entirely, you use the boosting query: it takes a required positive query and a negative query, and for any document that matches the negative clause, it multiplies its overall score by a negative_boost factor (a value between 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch('http://localhost:9200')\n",
    "index_name = \"apod\"\n",
    "\n",
    "# 1️⃣ Positive boost: increase score when explanation contains \"meteor\"\n",
    "#    Uses `boost` directly on a match query.\n",
    "print(\"=== Positive Boost: explanation contains 'meteor' (boost=2.0) ===\")\n",
    "pos_query = {\n",
    "    \"size\": 2,\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"explanation\": {\n",
    "                \"query\": \"meteor\",\n",
    "                \"boost\": 2.0      # positive boost on match\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "res = es.search(index=index_name, body=pos_query)\n",
    "for hit in res[\"hits\"][\"hits\"]:\n",
    "    print(f\"_score={hit['_score']:.2f}\") \n",
    "    pprint(hit[\"_source\"])\n",
    "\n",
    "# 2️⃣ Negative boost: de-emphasize docs where explanation contains \"comet\"\n",
    "#    Uses the boosting query with match_all as the positive clause.\n",
    "print(\"\\n=== Negative Boost: explanation contains 'comet' (negative_boost=0.5) ===\")\n",
    "neg_query = {\n",
    "    \"size\": 2,\n",
    "    \"query\": {\n",
    "        \"boosting\": {\n",
    "            \"positive\": { \"match_all\": {} },        # match everything\n",
    "            \"negative\": {                           # demote these\n",
    "                \"match\": { \"explanation\": \"comet\" }\n",
    "            },\n",
    "            \"negative_boost\": 0.5                   # reduce score by 50% on match\n",
    "        }\n",
    "    }\n",
    "}\n",
    "res = es.search(index=index_name, body=neg_query)\n",
    "for hit in res[\"hits\"][\"hits\"]:\n",
    "    print(f\"_score={hit['_score']:.2f}\")\n",
    "    pprint(hit[\"_source\"])\n",
    "\n",
    "# 3️⃣ Combined boost: promote \"meteor\" matches and demote \"comet\" matches\n",
    "print(\"\\n=== Combined Boost: +2.0 for 'meteor', -0.5 for 'comet' ===\")\n",
    "both_query = {\n",
    "    \"size\": 2,\n",
    "    \"query\": {\n",
    "        \"boosting\": {\n",
    "            \"positive\": {\n",
    "                \"match\": {\n",
    "                    \"explanation\": {\n",
    "                        \"query\": \"meteor\",\n",
    "                        \"boost\": 2.0\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"negative\": {\n",
    "                \"match\": {\n",
    "                    \"explanation\": \"comet\"\n",
    "                }\n",
    "            },\n",
    "            \"negative_boost\": 0.5\n",
    "        }\n",
    "    }\n",
    "}\n",
    "res = es.search(index=index_name, body=both_query)\n",
    "for hit in res[\"hits\"][\"hits\"]:\n",
    "    print(f\"_score={hit['_score']:.2f}\")\n",
    "    pprint(hit[\"_source\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we’ll dive into a handful of Elasticsearch’s most powerful advanced features. While Elasticsearch offers a wealth of capabilities beyond what we cover here, the three topics we’ll focus on are:\n",
    "\n",
    "- **Aggregations**: Real-time analytics and data summarization  \n",
    "- **Highlighting**: Extracting and emphasizing matching text snippets  \n",
    "- **Autocomplete**: Instant-search experiences via suggesters and search-as-you-type  \n",
    "\n",
    "Each feature can be mixed and matched or extended with dozens of other Elasticsearch tools to build rich, high-performance search applications tailored to your needs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregation queries don’t return individual documents, instead, they compute analytics over the set of matched records. Elasticsearch supports three main aggregation types:\n",
    "\n",
    "- **Bucket aggregations** group documents into “buckets” based on shared values (e.g., terms, date intervals, numeric ranges, or histograms).  \n",
    "- **Metric aggregations** calculate statistics (such as count, sum, average, min/max) over those documents.  \n",
    "- **Pipeline aggregations** take the output of one or more aggregations and run further calculations on it, enabling you to chain operations together.\n",
    "\n",
    "In this tutorial, we’ll use those aggregations to show how you can both segment your data and then perform successive analyses on those segments.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to get the number of documents per month\n",
    "# To do so, we will use a date_histogram aggregation to group documents by month\n",
    "# and a cumulative_sum aggregation to get the cumulative count of documents over time.\n",
    "body = {\n",
    "    \"size\": 0,\n",
    "    \"aggs\": {\n",
    "        \"entries_per_month\": {\n",
    "            # Bucket agg that uses a date_histogram to group documents by month\n",
    "            \"date_histogram\": {\n",
    "                \"field\":             \"date\",\n",
    "                \"calendar_interval\": \"month\",\n",
    "                \"format\":            \"yyyy-MM\"\n",
    "            },\n",
    "            # Pipeline agg that calculates the cumulative sum of the monthly counts\n",
    "            \"aggs\": {\n",
    "                # Metric agg that counts the number of documents in each month\n",
    "                \"monthly_count\": {\n",
    "                    \"value_count\": { \"field\": \"date\" }\n",
    "                },\n",
    "                # Pipeline agg that calculates the cumulative sum of the monthly counts\n",
    "                \"cumulative_entries\": {\n",
    "                    \"cumulative_sum\": {\n",
    "                        \"buckets_path\": \"monthly_count\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = es.search(index=\"apod\", body=body)\n",
    "\n",
    "for bucket in response[\"aggregations\"][\"entries_per_month\"][\"buckets\"]:\n",
    "    month = bucket[\"key_as_string\"]\n",
    "    count = bucket[\"monthly_count\"][\"value\"]\n",
    "    cum   = bucket[\"cumulative_entries\"][\"value\"]\n",
    "    print(f\"{month} → count: {count}, cumulative: {cum}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highlighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highlighting in Elasticsearch works by surrounding each occurrence of a query term in your document text with customizable tags (by default <em>/</em>, but you can use any HTML or marker you like). When you include a highlight section in your search request, Elasticsearch will:\n",
    "\n",
    "1. Analyze the specified field(s) to find where your query terms fall.\n",
    "2. Extract short snippets (fragments) around each match.\n",
    "3. Wrap each matching term in your chosen pre_tags and post_tags.\n",
    "4. Return those snippets alongside each hit in a top-level highlight block.\n",
    "\n",
    "This makes it easy to show users exactly where and in what context—their search terms appeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "  \"query\": {\n",
    "    \"match\": {\n",
    "      \"explanation\": \"comet\"\n",
    "    }\n",
    "  },\n",
    "  \"highlight\": {\n",
    "    # Customize the highlight tags\n",
    "    \"pre_tags\":  [\"<mark>\"],\n",
    "    \"post_tags\": [\"</mark>\"],\n",
    "    # Specify in wich fields we want to highlight\n",
    "    \"fields\": {\n",
    "      \"explanation\": {\n",
    "        # This skip the fragmentation of the text and return the whole text with each match highlighted\n",
    "        \"number_of_fragments\": 0, \n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "response = es.search(index=\"apod\", body=query)\n",
    "\n",
    "print(\"=== Highlighted Results ===\")\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(f\"Title: {hit['_source']['title']}\")\n",
    "    print(\"Highlighted explanation:\")\n",
    "    for fragment in hit[\"highlight\"][\"explanation\"]:\n",
    "        print(fragment)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final feature we’ll cover is autocomplete. Elasticsearch offers four different approaches, but we’ll choose the simplest one with the least setup: the search-as-you-type mechanism.\n",
    "\n",
    "The first step is to reindex your data to add the field required for this feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if es.indices.exists(index=\"apod_v2\"):\n",
    "    es.indices.delete(index=\"apod_v2\")  \n",
    "\n",
    "# New index mapping with search_as_you_type\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"title\": {\n",
    "                \"type\":\"search_as_you_type\", # Enable autocomplete search\n",
    "                \"max_shingle_size\": 3\n",
    "            },\n",
    "            \"date\":        { \"type\": \"date\",   \"format\": \"yyyy-MM-dd\" },\n",
    "            \"explanation\": { \"type\": \"text\" },\n",
    "            \"image_url\":   { \"type\": \"keyword\" },\n",
    "            \"authors\":     { \"type\": \"text\" }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "es.indices.create(index=\"apod_v2\", body=mapping)\n",
    "\n",
    "es.reindex(\n",
    "    body={\n",
    "        \"source\": { \"index\": \"apod\" },\n",
    "        \"dest\":   { \"index\": \"apod_v2\" }\n",
    "    },\n",
    "    wait_for_completion=True,\n",
    ")\n",
    "\n",
    "print(\"===Reindexing complete===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate search-as-you-type in this notebook, we’ll embed an ipywidgets.Combobox as our search bar. Under the hood, each time you type a character, a tiny Python function sends a bool_prefix query to our search_as_you_type index and updates the dropdown with the matching titles. Note that you might need to press the enter key once to activate the search bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets jupyterlab_widgets                      \n",
    "!jupyter nbextension enable --py --sys-prefix widgetsnbextension\n",
    "!pip install jupyterlab\n",
    "!jupyter labextension install @jupyter-widgets/jupyterlab-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realized with the help of chatGPT\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Create the Search bar\n",
    "combo = widgets.Combobox(\n",
    "    placeholder='Type to search titles…',\n",
    "    options=[],\n",
    "    description='Search:',\n",
    "    ensure_option=False,\n",
    "    continuous_update=True\n",
    ")\n",
    "display(combo)\n",
    "\n",
    "last_call = 0\n",
    "lock = threading.Lock()\n",
    "\n",
    "def fetch_suggestions(text):\n",
    "    # Elasticsearch bool_prefix query against search-as-you-type\n",
    "    body = {\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": text,\n",
    "                \"type\":  \"bool_prefix\",\n",
    "                \"fields\": [\"title\",\"title._2gram\",\"title._3gram\"]\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"title\"],\n",
    "        \"size\": 5\n",
    "    }\n",
    "    resp = es.search(index=\"apod_v2\", body=body)\n",
    "    return [hit[\"_source\"][\"title\"] for hit in resp[\"hits\"][\"hits\"]]\n",
    "\n",
    "def on_value_change(change):\n",
    "    global last_call\n",
    "    value = change['new']\n",
    "    now = time.time()\n",
    "    with lock:\n",
    "        if now - last_call < 0.3:\n",
    "            return\n",
    "        last_call = now\n",
    "    if value:\n",
    "        combo.options = fetch_suggestions(value)\n",
    "    else:\n",
    "        combo.options = []\n",
    "\n",
    "combo.observe(on_value_change, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations and comparison with SQL\n",
    "One of the most commonly mentioned limitations in forums and blogs is that Elasticsearch performance and data durability can be heavily constrained by the resources allocated to the cluster. To maintain good performance and ensure data survivability, it may be necessary to increase the number of nodes and replicate data across them. This is not a technical limitation per se, but rather an infrastructure constraint that depends on available resources.\n",
    "\n",
    "From a technical standpoint, Elasticsearch is built on top of the Lucene engine, which is optimized for full-text search rather than relational queries such as joins or transactions. While Elasticsearch does offer limited support for join-like operations, they are far less powerful than those available in SQL databases and are typically very resource-intensive. As a result, these features are often disabled in production configurations.\n",
    "\n",
    "In terms of consistency, Elasticsearch follows an eventual consistency model rather than strong consistency. This means that in certain race conditions, inconsistencies in query results can occur shortly after data is written or updated.\n",
    "\n",
    "Although a portion of Elasticsearch is available for free, accessing the full feature set—particularly advanced security, monitoring, and machine learning capabilities—requires a paid license.\n",
    "\n",
    "Nevertheless, Elasticsearch remains one of the most powerful and widely adopted solutions for full-text search, offering high performance, scalability, and a rich query language. Its ability to index and search large volumes of textual data in near real-time makes it a strong choice for use cases such as log analytics, product search, and document indexing.\n",
    "\n",
    "Alternatives to Elasticsearch include Apache Solr, which is also built on Lucene and excels in traditional search applications, and OpenSearch, a community-driven fork of Elasticsearch that retains many of its features under a fully open-source license.\n",
    "\n",
    "### Comparison between relational Databases and ElasticSearch\n",
    "As with most decisions in technology, selecting the appropriate tools hinges on your specific use case. It's essential to assess the features you require, understand your typical workflows, and determine the data handling properties that are most critical for your operations. The following table provides a comparative overview of two prominent technologies, aiding in an informed decision-making process.\n",
    "\n",
    "| **Aspect**                | **Elasticsearch**                                                                                                                             | **MySQL**                                                                                                                       |\n",
    "|---------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Data Model**            | Document-oriented (NoSQL); stores data as JSON documents.                                                                                     | Relational (SQL); stores data in structured tables with predefined schemas.                                                     |\n",
    "| **Primary Use Cases**     | Full-text search, log and event data analysis, real-time analytics, and applications requiring complex search capabilities.                   | Transactional applications, structured data storage, and scenarios requiring complex joins and ACID compliance.                 |\n",
    "| **Query Language**        | Elasticsearch Query DSL (Domain Specific Language); designed for flexible and complex search operations.                                      | SQL (Structured Query Language); widely adopted for structured data querying and manipulation.                                  |\n",
    "| **Joins & Relationships** | Limited support for joins; alternatives like nested documents and parent-child relationships exist but can be complex and resource-intensive. | Robust support for joins, foreign keys, and complex relational queries.                                                         |\n",
    "| **Schema Flexibility**    | Schema-less; allows dynamic mapping, making it adaptable to varying data structures.                                                          | Schema-based; requires predefined schemas, offering strict data validation and integrity.                                       |\n",
    "| **Consistency Model**     | Eventually consistent; suitable for scenarios where immediate consistency is not critical.                                                    | Strong consistency with ACID (Atomicity, Consistency, Isolation, Durability) properties, ensuring reliable transactions.        |\n",
    "| **Scalability**           | Horizontally scalable; designed to handle large volumes of data across distributed systems.                                                   | Vertically scalable; can be scaled horizontally with additional configurations but is primarily optimized for vertical scaling. |\n",
    "| **Performance**           | Optimized for search operations; excels in scenarios requiring rapid full-text search and analytics.                                          | Optimized for transactional operations; performs well in scenarios requiring complex transactions and data integrity.           |\n",
    "| **Security Features**     | Basic security features available; advanced features like role-based access control and encryption are part of the commercial offerings.      | Offers robust security features, including user authentication, SSL support, and role-based access control.                     |\n",
    "| **Licensing**             | Open-source with a dual license model; some advanced features require a commercial license.                                                   | Open-source (GPL) with commercial support available; widely adopted and supported by a large community.                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We began this tutorial by introducing the goals of Elasticsearch and how it leverages complex indexing mechanisms to deliver high performance and scalability. We also provided a brief recap of what indexes are, how they are used in relational databases, and the limitations and performance costs they can introduce.\n",
    "\n",
    "As we progressed, we walked through setting up an Elasticsearch cluster using Docker, connecting to it, and uploading bulk data. We then explored the basic API operations for document manipulation by ID and highlighted the differences between Query DSL and SQL. From there, we reviewed a broad range of advanced query types and features specifically tailored for building powerful search engine experiences.\n",
    "\n",
    "Using Elasticsearch proved to be quite intuitive. Even though we only covered a subset of its capabilities, it quickly became clear how the technology fits into real-world applications. Along the way, we encountered many configuration parameters that allow you to fine-tune Elasticsearch's behavior to meet specific needs.\n",
    "\n",
    "While Elasticsearch excels at full-text search, it may require more resources than a traditional database system. We also noted that its eventual consistency model differs from conventional databases, making it less suitable for some workflows. Nevertheless, if your application requires a robust and flexible full-text search engine, Elasticsearch is one of the most powerful solutions available, thanks to its rich feature set and high configurability.\n",
    "\n",
    "By following this tutorial, you should now have the knowledge and skills to build your first search engine application. With a thoughtfully designed search interface and parameterized queries, you can now deliver accurate and relevant results to your users efficiently and effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <a id=\"freecodecamp\"></a> [Elasticsearch Course for Beginners - FreeCodeCamp](https://www.youtube.com/watch?v=a4HBKEda_F8&ab_channel=freeCodeCamp.org)\n",
    "\n",
    "2. <a id=\"elasticdoc\"></a> [Elastic Official Documentation](https://www.elastic.co/docs/get-started)\n",
    "\n",
    "3. <a id=\"elasticlab\"></a> [Elastic Search Lab - Tutorials](https://www.elastic.co/search-labs/tutorials)\n",
    "\n",
    "4. <a id=\"elasticlabBoolQueries\"> [Elastic Search Lab - Tutortial - Filters](https://www.elastic.co/search-labs/tutorials/search-tutorial/full-text-search/filters)\n",
    "\n",
    "5. <a id=\"elasticscore\"></a> [Elastic Search Lab - Understanding Elasticsearch scoring and the Explain API](https://www.elastic.co/search-labs/blog/elasticsearch-scoring-and-explain-api)\n",
    "\n",
    "6. <a id=\"mustnot\"></a> [Soumendra - Stack Overflow - Difference between must_not and filter in elasticsearch](https://stackoverflow.com/questions/47226479/difference-between-must-not-and-filter-in-elasticsearch)\n",
    "\n",
    "7. <a id=\"aggregations\"></a> [logz.io - Daniel Berman - A Basic Guide To Elasticsearch Aggregations](https://logz.io/blog/elasticsearch-aggregations)\n",
    "\n",
    "8. <a id=\"highlighting\"></a> [Elastic Official Documentation - Highlighting](https://www.elastic.co/docs/reference/elasticsearch/rest-apis/highlighting)\n",
    "\n",
    "9. <a id=\"autocomplete\"></a> [Opster - Amit Khandelwal - Elasticsearch Autocomplete Search](https://opster.com/guides/elasticsearch/how-tos/elasticsearch-auto-complete-guide/)\n",
    "\n",
    "10. <a id=\"JoinQueries\"></a> [Elastic Official Documentation - Joining Queries](https://www.elastic.co/docs/reference/query-languages/query-dsl/joining-queries)\n",
    "\n",
    "11. <a id=\"ressourcesLimit\"></a> [Reddit - What are the limits of elastic search?](https://www.reddit.com/r/elasticsearch/comments/6xm8wv/what_are_the_limits_of_elastic_search/)\n",
    "\n",
    "12. <a id=\"ESalternative1\"></a> [sematext - 11 Alternatives to Elasticsearch, OpenSearch, and Solr](https://sematext.com/blog/elasticsearch-opensearch-solr-alternatives/)\n",
    "\n",
    "13. <a id=\"ESalternative2\"></a> [BIGDATA - Elasticsearch Alternatives - The Ultimate Guide](https://bigdataboutique.com/blog/elasticsearch-alternatives-the-ultimate-guide-59ad00)\n",
    "\n",
    "14. <a id=\"ESvsSQL1\"></a> [Medium - Elasticsearch vs. Traditional Databases: Diving into Elastic search's Strengths](https://medium.com/@rajeevprasanna/elasticsearch-vs-traditional-databases-diving-into-elastic-searchs-strengths-c6f55b9b449f)\n",
    "\n",
    "15. <a id=\"ESvsSQL2\"></a> [knowi - Elasticsearch vs. MySQL: What to Choose?](https://www.knowi.com/blog/elasticsearch-vs-mysql-what-to-choose/)\n",
    "\n",
    "16. <a id=\"TableGen\"></a> [Table Generator](https://www.tablesgenerator.com/markdown_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process and Work Distribution\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
